# MATCHING- AND POSELIB

- [Introduction](#introduction)
- [Supported Keypoint and Descriptor Types](#support-features)
- [Supported Matching Algorithms](#support-matching)
- [Supported Correspondence Filtering Techniques](#support-filtering)
- [Supported Pose Estimation Algorithms](#support-pose)
- [Installation](#installation)
    - [Dependencies](#dependencies)
        - [Docker](#docker)
        - [System-wide Installation on Linux Systems](#system-dependencies)
    - [Using Stand-Alone Executables without Docker](#executable)
    - [Library](#library)
- [Quick Start: Using Provided Test Data](#quick-start)
    - [Sparse Feature Matching](#quick-matching)
    - [Pose Estimation](#quick-pose)
- [Stand-Alone Executable for Feature Matching](#executable-matching)
- [Stand-Alone Executable for Pose Estimation](#executable-pose)
- [Stand-Alone Executable for Reading Test Data Generated by SemiRealSequence](#executable-test)
- [Interfacing the Library](#interface-lib)
    - [Calculation of Sparse Feature Matches](#interface-matching)
    - [Calculation of Relative Poses](#interface-pose)
    - [Continuous High Accuracy Stereo Pose Estimation](#interface-stereo)
    - [ROS](#interface-ros)
- [Testing Results on Supported Keypoint, Descriptor, and Matching Algorithm Types](#tests-features)
- [Testing Results on Supported Pose Estimation Algorithms](#tests-pose)
- [Publication](#publication)

## Introduction <a name="introduction"></a>

This library includes various algorithms for calculating, filtering, refining, and matching of sparse image features.
Moreover, multiple algorithms for estimating relative poses like different random sample consensus algorithms, 5pt solvers, cost functions, linear pose refinement algorithms, bundle adjustment (BA) options, and stereo rectification algorithms are provided.
For continuously estimating high accurate relative stereo poses, the library includes a framework that estimates stereo poses and continuously refines them using multiple stereo images (e.g. from a streaming stereo rig) while detecting pose changes to achieve pose accuracies comparable to offline calibration methods.

For easy interfacing, we provide 3 possibilities to interface the library:
* Stand-alone executables with a command-line interface to read image and calibration data from disk
* A library that can be integrated into your own application
* A ROS interface for reading continuous image data providing a launch file and the possibility to dynamically reconfigure parameters during runtime

We further provide testing results on various keypoint-descriptor combinations, matching algorithms, and pose estimation algorithms.

## Supported Keypoint and Descriptor Types <a name="support-features"></a>

Currently, all keypoint and descriptor types within [OpenCV](https://docs.opencv.org/4.2.0/d5/d51/group__features2d__main.html) (including [contrib](https://docs.opencv.org/4.2.0/d7/d7a/group__xfeatures2d__experiment.html)) in addition to [BOLD](https://github.com/vbalnt/bold) and [RIFF](http://press.liacs.nl/publications/RIFF%20-%20Retina-inspired%20Invariant%20Fast%20Feature%20Descriptor.pdf) are supported.

Keypoint types:
* FAST
* MSER
* ORB
* BRISK
* KAZE
* AKAZE
* STAR
* MSD
* SIFT
* SURF

Descriptor types:
* BRISK
* ORB
* KAZE
* AKAZE
* FREAK
* DAISY
* LATCH
* BGM
* BGM_HARD
* BGM_BILINEAR
* LBGM
* BINBOOST_64
* BINBOOST_128
* BINBOOST_256
* VGG_120
* VGG_80
* VGG_64
* VGG_48
* SIFT
* SURF
* RIFF
* BOLD

To enable SIFT and SURF, OpenCV must be built with enabled non-free code (contrib) and option `-DUSE_NON_FREE_CODE=ON` must be provided to CMake when building this library.

## Supported Matching Algorithms <a name="support-matching"></a>

* CASHASH:	    Cascade Hashing matcher from the [NMSLIB](https://github.com/nmslib/nmslib)
* GMBSOF:	    [Guided Matching based on Statistical Optical Flow](https://link.springer.com/chapter/10.1007/978-3-319-46478-7_7)
* HIRCLUIDX:    Hirarchical Clustering Index Matching from the [FLANN library](https://github.com/mariusmuja/flann)
* HIRKMEANS:    Hierarchical k-means tree matcher from the [FLANN library](https://github.com/mariusmuja/flann)
* LINEAR:	    Linear matching algorithm (Brute force) from the [FLANN library](https://github.com/mariusmuja/flann)
* LSHIDX:	    LSH Index Matching algorithm from the [FLANN library](https://github.com/mariusmuja/flann)
* RANDKDTREE:	Randomized KD-trees matcher from the [FLANN library](https://github.com/mariusmuja/flann)
* SWGRAPH:	    Small World Graph (SW-graph) from the [NMSLIB](https://github.com/nmslib/nmslib)
* HNSW:         Hiarchical Navigable Small World Graph from the [NMSLIB](https://github.com/nmslib/nmslib)
* VPTREE:       VP-tree or ball-tree from the [NMSLIB](https://github.com/nmslib/nmslib)
* MVPTREE:      Multi-Vantage Point Tree from the [NMSLIB](https://github.com/nmslib/nmslib)
* GHTREE:       GH-Tree from the [NMSLIB](https://github.com/nmslib/nmslib)
* LISTCLU:      List of clusters from the [NMSLIB](https://github.com/nmslib/nmslib)
* SATREE:       Spatial Approximation Tree from the [NMSLIB](https://github.com/nmslib/nmslib).
* BRUTEFORCENMS: Brute-force (sequential) searching from the [NMSLIB](https://github.com/nmslib/nmslib)
* ANNOY:        [Approximate Nearest Neighbors Matcher](https://github.com/spotify/annoy)
* LKOF:         Lucas Kanade Optical Flow
* LKOFT:        Lucas Kanade Optical Flow Tracker
* ALKOF:        Advanced Lucas Kanade Optical Flow
* ALKOFT:       Advanced Lucas Kanade Optical Flow Tracker

## Supported Correspondence Filtering Techniques <a name="support-filtering"></a>

After matching, we provide possibilities to filter found matches:
* Descriptor distance ratio filter (Lowe's ratio test)
* GMS: [Grid-based Motion Statistics for Fast, Ultra-robust Feature Correspondence](https://github.com/JiawangBian/GMS-Feature-Matcher)
* VFC: [Vector Field Consensus](https://github.com/jiayi-ma/VFC)
* SOF: [Statistical Optical Flow](https://link.springer.com/chapter/10.1007/978-3-319-46478-7_7)

## Supported Pose Estimation Algorithms <a name="support-pose"></a>

This library supports multiple algorithms and combinations thereof to estimate relative poses between cameras.
We included several ***random sample consensus algorithms***:
* RANSAC
* [USAC](https://ieeexplore.ieee.org/document/6365642) (adapted to estimate PROSAC beta and SPRT delta)
* [ARRSAC](https://github.com/rust-cv/arrsac)
* [MLESAC](http://www.robots.ox.ac.uk/~vgg/publications/papers/torr00.pdf)
* [NG-RANSAC](https://github.com/vislearn/ngransac) (not included in this repository but integrated in the [testing-version of this library](https://github.com/josefmaierfl/autocalib_test_package/tree/ngransac))

USAC additionally supports the ***detection and correction of degenerate cases*** (i.e. rotation only) using [QDEGSAC](https://people.inf.ethz.ch/pomarc/pubs/QDEGSAC.pdf) or a USAC-internal solution.

The following algorithms for ***pose estimation*** can be used within RANSAC variants:
* 5pt solvers (some of them from [OpenGV](http://laurentkneip.github.io/opengv/))
    * [Nister](http://www.ee.oulu.fi/research/imag/courses/Sturm/nister04.pdf)
    * [Stewenius](http://www.robots.ox.ac.uk/~vgg/publications/papers/stewenius05a.pdf)
    * [Kneip](http://laurentkneip.github.io/opengv/)
* [8pt algorithm](http://www.cs.cmu.edu/afs/andrew/scs/cs/15-463/f07/proj_final/www/amichals/fundamental.pdf)
* Homography alignment

For each of the afore mentioned algorithms different ***cost functions*** can be chosen:
* Squared norm
* Torr weights ([Torr dissertation](http://www.robots.ox.ac.uk/~phst/Papers/SPIE93/m.ps.gz), Eqn. 2.25)
* [Pseudo-Huber](https://en.wikipedia.org/wiki/Huber_loss)

Robustly estimated minimal sample models (i.e. Essential matrix or rotation matrix and translation vector) can be subsequently ***refined*** utilizing all inliers by a combination of above mentioned solvers and cost functions.

In addition, models can be refined by applying ***bundle adjustment (BA)*** on extrinsics only or on both, extrinsics and intrinsics.

We further implemented a ***linear rectification algorithm*** for general, unconstrained stereo rigs based on the publication ["A compact algorithm for rectification of stereo pairs"](https://link.springer.com/article/10.1007/s001380050120).

All above mentioned methods, algorithms, and cost functions can further be used within a ***continious high accurate relative stereo pose estimation framework*** which estimates a relative stereo pose and refines it utilizing additional stereo image pairs by concatenating the "best" correspondences over multiple stereo frames while detecting pose changes.

## Installation <a name="installation"></a>

The library was tested on Ubuntu and Windows but we provide a [Docker](https://docs.docker.com/get-docker/) file for operating system independent usage.
We provide multiple possibilities to use this software:
* Stand-alone executables to
    * Calculate image features and perform matching based on images stored on disk
    * Calculate relative poses (mono and stereo cameras) based on images and calibration (camera intrinsics) info (based on [KITTI format](https://s3.eu-central-1.amazonaws.com/avg-kitti/devkit_raw_data.zip)) stored on disk
    * Test algorithms based on GT data generated by [SemiRealSequence](https://github.com/josefmaierfl/SemiRealSequence)
* [Docker](https://docs.docker.com/get-docker/) image with afore mentioned stand-alone executables
* [ROS interface](https://github.com/josefmaierfl/matchinglib_poselib_ros)
* Library for integration into your own application

### Dependencies <a name="dependencies"></a>

#### Docker <a name="docker"></a>

If you only want to use the provided executables, [Docker](https://docs.docker.com/get-docker/) can be used.
After installing Docker, the corresponding Docker image can be built by executing `./build_docker_base.sh` in the main directory of this repository.
On Windows, the image can be built by executing `docker build -t poselib:1.0 .` in the main directory of this repository using Powershell.

#### System-wide Installation on Linux Systems <a name="system-dependencies"></a>

SemiRealSequence depends on the following libraries:
* [Eigen 3.3.7](http://eigen.tuxfamily.org/index.php?title=Main_Page)
* [Boost](https://www.boost.org/)
* [OpenCV 4.2.0](https://opencv.org/)
* [SBA](https://github.com/balintfodor/sba)
* [CLAPACK](https://github.com/NIRALUser/CLAPACK)

For installing above libraries, the following packages should be installed:
```bash
sudo apt-get update
sudo apt-get install libboost-all-dev
sudo apt-get install build-essential cmake pkg-config
sudo apt-get install wget \
    libtbb2 \
    libtbb-dev \
    libglew-dev \
    qt5-default \
    libxkbcommon-dev \
    libflann-dev \
    libpng-dev \
    libgtk-3-dev \
    libgtkglext1 \
    libgtkglext1-dev \
    libtiff-dev \
    libtiff5-dev \
    libtiffxx5 \
    libjpeg-dev \
    libjasper1 \
    libjasper-dev \
    libavcodec-dev \
    libavformat-dev \
    libswscale-dev \
    libv4l-dev \
    libxvidcore-dev \
    libx264-dev \
    libdc1394-22-dev \
    openexr \
    libatlas-base-dev \
    gfortran \
    libhdf5-dev
sudo apt-get install libglu1-mesa-dev mesa-common-dev mesa-utils freeglut3-dev
sudo apt-get install libomp-dev
```
All mentioned libries (OpenCV, ...) can be installed by executing `./build_thirdparty.sh` within directory `ci` of this repository.
If some of the libraries are already installed on your system, missing libraries can be installed using the corresponding script file within directory [ci](./ci).
CLAPACK and SBA can be installed by executing commands below within directory [ci](./ci):
```bash
thirdparty_dir="$(pwd)/thirdparty"
cd ${thirdparty_dir}/clapack-3.2.1/build/generic
./build.sh
cd ${thirdparty_dir}/sba-1.6/build/generic
./build.sh
```

Libraries integrated into this library (no need for installation):
* [FLANN library](https://github.com/mariusmuja/flann)
* [NMSLIB](https://github.com/nmslib/nmslib)
* [ANNOY](https://github.com/spotify/annoy)
* [nanoflann](https://github.com/jlblancoc/nanoflann)
* [GMS](https://github.com/JiawangBian/GMS-Feature-Matcher)
* [VFC](https://github.com/jiayi-ma/VFC)
* [BOLD](https://github.com/vbalnt/bold)
* [RIFF](https://press.liacs.nl/researchdownloads/riff/Descriptor_Project(mm2014).zip)
* [ARRSAC](https://github.com/rust-cv/arrsac)
* [USAC](http://www.cs.unc.edu/~rraguram/usac/)
* [OpenGV](http://laurentkneip.github.io/opengv/)

### Using Stand-Alone Executables without Docker <a name="executable"></a>

After [installing dependencies](#system-dependencies), the library and executables can be built by executing `./build_install_matchinglib_poselib.sh exe` (no system-wide installation) or by (within main repository directory)
```bash
cd matchinglib_poselib
mkdir build
cd build
cmake .. -DCMAKE_BUILD_TYPE=Release -DOPTION_BUILD_TESTS=ON
make -j "$(nproc)"
```
Executables for matching features `matchinglib-test`, pose estimation `poselib-test`, and testing `noMatch_poselib-test` are located in the build folder.

### Library <a name="library"></a>

To use the library within your own application, install all necessary [dependencies](#system-dependencies) and subsequently build and install it by calling

`./build_install_matchinglib_poselib.sh install`

or by performing the following steps:

```bash
cd matchinglib_poselib
mkdir build
cd build
cmake .. -DCMAKE_BUILD_TYPE=Release -DOPTION_BUILD_TESTS=OFF -DBUILD_SHARED_LIBS=ON
make -j "$(nproc)"
sudo make install
```

To integrate it within CMake include the following into your `CMakeLists.txt`:

```
find_package(matchinglib_poselib REQUIRED)
find_package(Eigen REQUIRED)
find_package(OpenCV 4.2.0 REQUIRED)

target_link_libraries(your_project_name
  ${OpenCV_LIBS}
  sba
  ${CLAPACK_LIBRARIES}
  matchinglib_poselib::matchinglib
  matchinglib_poselib::poselib)

target_include_directories(your_project_name
  PRIVATE
  ${DEFAULT_INCLUDE_DIRECTORIES}
  ${OpenCV_INCLUDE_DIRS}
  ${Eigen_INCLUDE_DIR}
  ${CLAPACK_INCLUDE_DIRS}
  matchinglib_poselib::matchinglib
  matchinglib_poselib::poselib)
```

## Quick Start: Using Provided Test Data <a name="quick-start"></a>

This repository includes a small test data set (images and intrinsics calibration information) which is copied into the build directory during the CMake build process.
This can be disabled by providing options `-DCOPY_TEST_MATCH_IMGS=OFF` and/or `COPY_TEST_POSE_IMGS=OFF` to CMake.

### Sparse Feature Matching <a name="quick-matching"></a>

To start extracting features and calculating matches utilizing some default parameters call `./matchinglib-test` within your build directory or, if you are using Docker, run `./run_docker_base.sh match` within the main directory of this repository.
This will show a subset of found matches for every image pair.

To change parameters and/or the directory containing images execute `./matchinglib-test -h` or `./run_docker_base.sh match -h` to show options.

### Pose Estimation <a name="quick-pose"></a>

To start extracting features, calculating matches, and estimating poses utilizing some default parameters call `./poselib-test` within your build directory or, if you are using Docker, run `./run_docker_base.sh pose` within the main directory of this repository.
This will show a subset of found matches and rectified images for every input image pair.
The accuracy of estimated poses and rectification can be checked by moving your mouse cursor on the rectified images.

To change parameters and/or the directory containing images execute `./poselib-test -h` or `./run_docker_base.sh pose -h` to show options.

## Stand-Alone Executable for Feature Matching <a name="executable-matching"></a>

For testing executbale `matchinglib-test` with default parameters and the provided test images see [here](#quick-matching).

Feature matches can be calculated using all available algorithms (see [here](#support-features), [here](#support-matching), and [here](#support-filtering)).
Algorithm names can be specified as strings like listed [here](#support-features) and [here](#support-matching) with command line options `--f_detect`, `--d_extr`, and `--matcher`.

Feature matches can be calculated for multiple mono and stereo camera images located within a single folder by either providing only a pre- and/or post-fix using `--l_img_pref` for one image sequence or by providing 2 pre- and/or post-fixes using `--l_img_pref` and `--r_img_pref` for stereo image sequences, respectively.
By default, matches are not stored but displayed.
To store keypoints and matches provide option `--output_path`.
To disable displaying found matches provide option `--showNr -3`.

To start calculating features and matches call `./matchinglib-test [options]` or `./run_docker_base.sh match [options]`.

For additional options and details call `./matchinglib-test -h` or `./run_docker_base.sh match -h`.

## Stand-Alone Executable for Pose Estimation <a name="executable-pose"></a>

For testing executbale `poselib-test` with default parameters and the provided test data see [here](#quick-pose).

Executable `poselib-test` supports the calculation of feature matches with identical options as for executbale `matchinglib-test` (see [here](#executable-matching)) with subsequent relative pose estimation.
All available algorithms described/listed [here](#support-pose) can be used.
For details call `./poselib-test -h` or `./run_docker_base.sh pose -h`.

Realtive poses can be calculated for multiple mono and stereo camera images located within a single folder by either providing only a pre- and/or post-fix using `--l_img_pref` for one image sequence or by providing 2 pre- and/or post-fixes using `--l_img_pref` and `--r_img_pref` for stereo image sequences, respectively.
For mono camera configurations (frame-to-frame poses and rectified images are calculated), a step size providing option `--stepSize` can be specified.
In addition, intrinsics calibration data (or at least an estimate with enabled bundle adjustment) must be provided.
This executable only supports reading intrinsics data for at most 2 cameras (mono, stereo).
Intrinsics data must be provided with a text file within the provided image directory.
The data format corresponds to [raw KITTI calibration format](https://s3.eu-central-1.amazonaws.com/avg-kitti/devkit_raw_data.zip).
The text file must also include extrinsics data which can be set to zero translation and identity rotation matrix.
Extrinsics data is only used for comparison (which can be disabled) with estimated poses.

To store rectified images provide option `--output_path`.
To disable displaying found matches, informational output, and rectified images provide options `--v 0 --showNr -3`.

To enable the continious high accurate relative stereo pose estimation which calculates poses based on multiple stereo frames that have identical intrinsics and extrinsics for at least a few frames, provide option `--stereoRef`.

To start calculating features, matches and relative poses call `./poselib-test [options]` or `./run_docker_base.sh pose [options]`.

For additional options and details call `./poselib-test -h` or `./run_docker_base.sh pose -h`.

## Stand-Alone Executable for Reading Test Data Generated by SemiRealSequence <a name="executable-test"></a>

Executable `noMatch_poselib-test` can be used for evaluation of all pose estimation related (excluding algorithms for calculating image features and matches) algorithms within this library.
Parameters are basically equal to executable `poselib-test` without parameters for feature calculation and matching.

Input are 3D data and correspondences generated by [SemiRealSequence](https://github.com/josefmaierfl/SemiRealSequence) which allows testing with GT data and for different 3D and correspondence data properties.
This enables finding optimal algorithm and parameter combinations for specific or general scene, pose, and camera properties.

We performed testing on over a million test scenarios and algorithm configurations using [SemiRealSequence](https://github.com/josefmaierfl/SemiRealSequence) and this library.
The full test framework can be found [here](https://github.com/josefmaierfl/autocalib_test_package).
Additional tests using [NG-RANSAC](https://github.com/vislearn/ngransac) which was integrated into an adapted version of executable `poselib-test` were performed using a [slightly adapted test framework](https://github.com/josefmaierfl/autocalib_test_package/tree/ngransac) to support [PyTorch](https://pytorch.org/).
For testing results, see [here](#tests-pose).

Sequences generated by [SemiRealSequence](https://github.com/josefmaierfl/SemiRealSequence) can be loaded by providing the path to generated 3D data with option `--sequ_path` in addition to option `--matchData_idx` with a number (e.g. 0, 1, 2, 3, ...) of sequentially generated correspondence data with different properties for the same 3D data.
If e.g. only one set of correspondence data was generated, option `--matchData_idx 0` must be provided.
Moreover, option `--ovf_ext` which specifies the data format and file extension (Possible options: yaml/xml/yaml.gz/xml.gz) of data to read must be provided.
Results of performed tests are stored to disk in CSV format within the directory provided by option `--output_path`.

Information on how to use [SemiRealSequence](https://github.com/josefmaierfl/SemiRealSequence) can be found in the corresponding [repository](https://github.com/josefmaierfl/SemiRealSequence).

## Interfacing the Library <a name="interface-lib"></a>

The library is split into 2 main parts: `matchinglib` and `poselib`.
To use them, install all [required dependencies](#system-dependencies) and [the library itself](#library) and [adopt your CMake](#library).

### Calculation of Sparse Feature Matches <a name="interface-matching"></a>

After loading 2 grayscale images `img1` and `img2` in OpenCV format (`cv::Mat`), matches can be calculated by calling function
```
int matchinglib::getCorrespondences(cv::Mat& img1,
    cv::Mat& img2,
    std::vector<cv::DMatch> & finalMatches,
    std::vector<cv::KeyPoint> & kp1,
    std::vector<cv::KeyPoint> & kp2,
    std::string featuretype = "FAST",
    std::string extractortype = "FREAK",
    std::string matchertype = "GMBSOF",
    bool dynamicKeypDet = true,
    int limitNrfeatures = 8000,
    bool VFCrefine = false,
    bool GMSrefine = false,
    bool ratioTest = true,
    bool SOFrefine = false,
    int subPixRefine = 0,
    int verbose = 0,
    std::string idxPars_NMSLIB = "",
    std::string queryPars_NMSLIB = "");
```
located in [./matchinglib_poselib/source/matchinglib/include/matchinglib/matchinglib_correspondences.h](./matchinglib_poselib/source/matchinglib/include/matchinglib/matchinglib_correspondences.h).
Include it with `#include "matchinglib/matchinglib.h"`.

Input variable descriptions:
* `cv::Mat img1`: First loaded grayscale image
* `cv::Mat img2`: Second loaded grayscale image
* `std::string featuretype`: Keypoint type as listed [here](#support-features)
* `std::string extractortype`: Descriptor type as listed [here](#support-features)
* `std::string matchertype`: Matching algorithm to use as listed [here](#support-matching)
* `bool dynamicKeypDet`: If true, found keypoints are filtered by local keypoint responses (i.e. [cornerness and blob strength](https://docs.opencv.org/4.2.0/d2/d29/classcv_1_1KeyPoint.html#a1f163ac418c281042e28895b20514360)) using an image grid to limit the number of keypoints to `limitNrfeatures`.
* `int limitNrfeatures`: Limit on the number keypoints for each image
* `bool VFCrefine`: Use the [VFC](https://github.com/jiayi-ma/VFC) algorithm to filter keypoints
* `bool GMSrefine`: Use the [GMS](https://github.com/JiawangBian/GMS-Feature-Matcher) algorithm to filter keypoints
* `bool ratioTest`: Use Lowe's ratio test
* `bool SOFrefine`: Use the [SOF](https://link.springer.com/chapter/10.1007/978-3-319-46478-7_7) algorithm to filter keypoints
* `int subPixRefine`: Use local patch matching to refine found keypoint locations
* `int verbose`: Verbosity level
* `std::string idxPars_NMSLIB`: Optional parameters for matching algorithms of the integrated [NMSLIB](https://github.com/nmslib/nmslib). See their documentation for details.
* `std::string queryPars_NMSLIB`: Optional parameters for matching algorithms of the integrated [NMSLIB](https://github.com/nmslib/nmslib). See their documentation for details.

Output variable descriptions:
* `std::vector<cv::DMatch> finalMatches`: Found matches in [OpenCV format](https://docs.opencv.org/4.2.0/d4/de0/classcv_1_1DMatch.html)
* `std::vector<cv::KeyPoint> kp1`: Found (and optionally filtered) keypoints in first image in [OpenCV format](https://docs.opencv.org/4.2.0/d2/d29/classcv_1_1KeyPoint.html)
* `std::vector<cv::KeyPoint> kp2`: Found (and optionally filtered) keypoints in second image in [OpenCV format](https://docs.opencv.org/4.2.0/d2/d29/classcv_1_1KeyPoint.html)

An example can be found in [./matchinglib_poselib/source/tests/matchinglib-test/main.cpp](./matchinglib_poselib/source/tests/matchinglib-test/main.cpp).

### Calculation of Relative Poses <a name="interface-pose"></a>

#### Preliminaries <a name="interface-pose-preliminaries"></a>

Using matches (e.g. calculated using function `matchinglib::getCorrespondences`), the follwing steps should be performed:
```
//Extract coordinates from keypoints
std::vector<cv::Point2f> points1, points2;
for (auto &j : finalMatches)
{
    points1.push_back(kp1[j.queryIdx].pt);
    points2.push_back(kp2[j.trainIdx].pt);
}

//Transfer into camera coordinates
poselib::ImgToCamCoordTrans(points1, K0);
poselib::ImgToCamCoordTrans(points2, K1);

//Undistort
poselib::Remove_LensDist(points1, points2, dist0_8, dist1_8);
```

Function `poselib::ImgToCamCoordTrans(std::vector<cv::Point2f>& points, cv::Mat K)` transfers matching keypoint locations `points1` and `points2` from the image into the camera coordinate system using camera matrices of first and second cameras, `cv::Mat K0` and `cv::Mat K1`, respectively.
Function `poselib::ImgToCamCoordTrans` can be found in [./matchinglib_poselib/source/poselib/include/poselib/pose_helper.h](./matchinglib_poselib/source/poselib/include/poselib/pose_helper.h) and can be included with `#include "poselib/pose_helper.h"`.
Function `poselib::Remove_LensDist(std::vector<cv::Point2f>& points1, std::vector<cv::Point2f>& points2, const cv::Mat& dist1, const cv::Mat& dist2)` can be used to remove radial distortion and is located in `pose_helper.h`.
Radial distortion parameters `dist1` and `dist2` correspond to [OpenCV format](https://docs.opencv.org/3.4.9/d9/d0c/group__calib3d.html#ga1019495a2c8d1743ed5cc23fa0daff8c) with 5 or 8 distortion parameters.

Based on distortion-free correspondences in camera coordinates, an Essential matrix can be estimated by one of [above listed robust methods](#support-pose).

#### USAC <a name="interface-pose-usac"></a>

In case you want to use [USAC](http://www.cs.unc.edu/~rraguram/usac/) call function
```
int estimateEssentialOrPoseUSAC(const cv::Mat & p1,
    const cv::Mat & p2,
    cv::OutputArray E,
    double th,
    ConfigUSAC & cfg,
    bool & isDegenerate,
    cv::OutputArray inliers = cv::noArray(),
    cv::OutputArray R_degenerate = cv::noArray(),
    cv::OutputArray inliers_degenerate_R = cv::noArray(),
    cv::OutputArray R = cv::noArray(),
    cv::OutputArray t = cv::noArray(),
    bool verbose = false)
```
located within [./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h](./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h).
It can be included with `#include "poselib/pose_estim.h"`.

Input variable descriptions:
* `cv::Mat p1`: (rows x cols) = ( n x 2 ) sized array (type `CV_64FC1`) of distortion-free keypoint locations in camera coordinates corresponding to `cv::Mat p2`
* `cv::Mat p2`: (rows x cols) = ( n x 2 ) sized array (type `CV_64FC1`) of distortion-free keypoint locations in camera coordinates corresponding to `cv::Mat p1`
* `double th`: Threshold in camera coordinates. It can be converted from image coordinates using <img src="https://render.githubusercontent.com/render/math?math=t\!h_{cam} = t\!h_{pix}\frac{4}{\sqrt{2}\left(f_{1,x} %2B f_{1,y} %2B f_{2,x} %2B f_{2,y}\right)}"> with focal lengths *f* of first and second camera in x- and y- direction
* `ConfigUSAC cfg`: USAC configuration parameters. See [./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h](./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h)
* `bool verbose`: Verbosity

Output variable descriptions:
* `cv::OutputArray E`: Essential matrix in `cv::Mat` format and type `CV_64FC1`
* `bool isDegenerate`: Returns true if solution is degenerate (i.e. rotation only)
* `cv::OutputArray inliers`: Inlier mask of size (rows x cols) = (1 x n) in `cv::Mat` format and type `CV_8UC1`
* `cv::OutputArray R_degenerate`: Rotation matrix if `isDegenerate=true`
* `cv::OutputArray inliers_degenerate_R`: Inlier mask (`cv::Mat` format and type `CV_8UC1`) for degenerate solution if `isDegenerate=true`
* `cv::OutputArray R`: Rotation matrix in case Kneip's Eigen solver was used in `cv::Mat` format and type `CV_64FC1`
* `cv::OutputArray t`: Translation vector of size (rows x cols) = (3 x 1) in `cv::Mat` format and type `CV_64FC1` in case Kneip's Eigen solver was used

#### RANSAC, ARRSAC, and MLESAC <a name="interface-pose-robust"></a>

In case you want to use vanilla RANSAC, ARRSAC, or MLESAC call function
```
bool poselib::estimateEssentialMat(cv::OutputArray E,
    cv::InputArray p1,
    cv::InputArray p2,
    const std::string &method,
    double threshold,
    bool refine,
    cv::OutputArray mask = cv::noArray());
```
located within [./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h](./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h).
It can be included with `#include "poselib/pose_estim.h"`.

Input variable descriptions:
* `cv::InputArray p1`: (rows x cols) = ( n x 2 ) sized array (type `CV_64FC1`) of distortion-free keypoint locations in camera coordinates corresponding to `cv::InputArray p2`
* `cv::InputArray p2`: (rows x cols) = ( n x 2 ) sized array (type `CV_64FC1`) of distortion-free keypoint locations in camera coordinates corresponding to `cv::InputArray p1`
* `std::string method`: Robust method to use: `RANSAC`, `ARRSAC`, or `MLESAC`
* `double threshold`: Threshold in camera coordinates. It can be converted from image coordinates using <img src="https://render.githubusercontent.com/render/math?math=t\!h_{cam} = t\!h_{pix}\frac{4}{\sqrt{2}\left(f_{1,x} %2B f_{1,y} %2B f_{2,x} %2B f_{2,y}\right)}"> with focal lengths *f* of first and second camera in x- and y- direction
* `bool refine`: If true, the found solution is refined using the 8pt algorithm and Pseudo-Huber weights (only for RANSAC and ARRSAC). The used refinement performs additional checks and optimizations. Better performance can be achieved in most cases if the [refinement described below](#interface-pose-linear-refinement) is used instead.

Output variable descriptions:
* `cv::OutputArray E`: Essential matrix in `cv::Mat` format and type `CV_64FC1`
* `cv::OutputArray mask`: Inlier mask of size (rows x cols) = (1 x n) in `cv::Mat` format and type `CV_8UC1`

#### Linear Refinement <a name="interface-pose-linear-refinement"></a>

Robustly estimated essential matrices can be linearly refined using

```
bool poselib::refineEssentialLinear(cv::InputArray p1,
    cv::InputArray p2,
    cv::InputOutputArray E,
    cv::InputOutputArray mask,
    int refineMethod,//a combination of poselib::RefinePostAlg
    size_t & nr_inliers,
    cv::InputOutputArray R = cv::noArray(),
    cv::OutputArray t = cv::noArray(),
    double th = 0.008,
    size_t num_iterative_steps = 4,
    double threshold_multiplier = 2.0,
    double pseudoHuberThreshold_multiplier = 0.1,
    double maxRelativeInlierCntLoss = 0.15);
```
located within [./matchinglib_poselib/source/poselib/include/poselib/pose_linear_refinement.h](./matchinglib_poselib/source/poselib/include/poselib/pose_linear_refinement.h).
It can be included with `#include "poselib/pose_linear_refinement.h"`.

Input variable descriptions:
* `cv::InputArray p1`: (rows x cols) = ( n x 2 ) sized array (type `CV_64FC1`) of distortion-free keypoint locations in camera coordinates corresponding to `cv::InputArray p2`
* `cv::InputArray p2`: (rows x cols) = ( n x 2 ) sized array (type `CV_64FC1`) of distortion-free keypoint locations in camera coordinates corresponding to `cv::InputArray p1`
* `int refineMethod`: Refinement method: A combination of `enum poselib::RefinePostAlg` located in [./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h](./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h).
Refinement algorithms `PR_8PT`, `PR_NISTER`, `PR_STEWENIUS` or `PR_KNEIP` can be combined with one of cost functions `PR_TORR_WEIGHTS`, `PR_PSEUDOHUBER_WEIGHTS`, or `PR_NO_WEIGHTS` (least squares solution) using the `|` (or) operator.
* `double th`:  Threshold in camera coordinates. It can be converted from image coordinates using <img src="https://render.githubusercontent.com/render/math?math=t\!h_{cam} = t\!h_{pix}\frac{4}{\sqrt{2}\left(f_{1,x} %2B f_{1,y} %2B f_{2,x} %2B f_{2,y}\right)}"> with focal lengths *f* of first and second camera in x- and y- direction
* `size_t num_iterative_steps`: Number of refinement steps *s* performed while iteratively changing the threshold to achieve the highest possible inlier count and accuracy
* `double threshold_multiplier`: Threshold multiplier *m* is used to calculate the utilized threshold <img src="https://render.githubusercontent.com/render/math?math=t\!h_{use} = m d - (i %2B 1) t\!h_{step}"> with threshold step size <img src="https://render.githubusercontent.com/render/math?math=t\!h_{step} = \frac{m d - d}{s}">, internal threshold <img src="https://render.githubusercontent.com/render/math?math=d = t\!h_{cam}^{2}">, number of refinement steps *s*, and iteration number *i*. The calculated threshold is used to determine the inliers after the new model was calculated.
* `double pseudoHuberThreshold_multiplier`: Multiplication factor for the threshold `double th` in case the Pseudo-Huber cost function is used as the Pseudo-Huber threshold should typically be smaller than the normal threshold
* `double maxRelativeInlierCntLoss`: Maximum allowed relative inlier loss during iterative refinement

Input/Output variable descriptions:
* `cv::InputOutputArray E`: Input: Essential matrix (`cv::Mat` of type `CV_64FC1`) from robust estimation; Output: Refined Essential matrix
* `cv::InputOutputArray mask`: Inlier mask of size (rows x cols) = (1 x n) in `cv::Mat` format and type `CV_8UC1`
* `cv::InputOutputArray R`: Rotation matrix in case Kneip's Eigen solver is used

Output variable descriptions:
* `cv::OutputArray t`: Translation vector (`cv::Mat` of type `CV_64FC1`) in case Kneip's Eigen solver is used

#### Calculation of R & t <a name="interface-pose-rt"></a>

For calculating the pose (i.e. rotation matrix R and translation vector t) and triangulated 3D points use function
```
int poselib::getPoseTriangPts(cv::InputArray E,
    cv::InputArray p1,
    cv::InputArray p2,
    cv::OutputArray R,
    cv::OutputArray t,
    cv::OutputArray Q,
    cv::InputOutputArray mask = cv::noArray(),
    const double dist = 50.0,
    bool translatE = false);
```
located within [./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h](./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h).
It can be included with `#include "poselib/pose_estim.h"`.

Input variable descriptions:
* `cv::InputArray E`: Essential matrix (`cv::Mat` of type `CV_64FC1`)
* `cv::InputArray p1`: (rows x cols) = ( n x 2 ) sized array (type `CV_64FC1`) of distortion-free keypoint locations in camera coordinates corresponding to `cv::InputArray p2`
* `cv::InputArray p2`: (rows x cols) = ( n x 2 ) sized array (type `CV_64FC1`) of distortion-free keypoint locations in camera coordinates corresponding to `cv::InputArray p1`
* `double dist`: Threshold on 3D coordinate depth values (z) based on a normalized translation vector (i.e. base length between cameras is 1)
* `bool translatE`: Set to true, if the provided Essential matrix corresponds to a translational essential matrix (R corresponds to identity)

Input/Output variable descriptions:
* `cv::InputOutputArray mask`: Inlier mask of size (rows x cols) = (1 x n) in `cv::Mat` format and type `CV_8UC1`

Output variable descriptions:
* `cv::OutputArray R`: Rotation matrix (`cv::Mat` of type `CV_64FC1`)
* `cv::OutputArray t`: Translation vector (`cv::Mat` of type `CV_64FC1`)
* `cv::OutputArray Q`: Triangulated 3D coordinates of size (rows x cols) = ( n x 3 ) and type `CV_64FC1` (`cv::Mat`)

For only triangulating 3D points with known R & t use
```
int poselib::triangPts3D(cv::InputArray R,
    cv::InputArray t,
    cv::InputArray _points1,
    cv::InputArray _points2,
    cv::OutputArray Q3D,
    cv::InputOutputArray mask = cv::noArray(),
    const double dist = 50.0);
```
located within [./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h](./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h).
It can be included with `#include "poselib/pose_estim.h"`.

Input variable descriptions:
* `cv::InputArray R`: Rotation matrix (`cv::Mat` of type `CV_64FC1`)
* `cv::InputArray t`: Translation vector (`cv::Mat` of type `CV_64FC1`)
* `cv::InputArray _points1`: (rows x cols) = ( n x 2 ) sized array (type `CV_64FC1`) of distortion-free keypoint locations in camera coordinates corresponding to `cv::InputArray _points2`
* `cv::InputArray _points2`: (rows x cols) = ( n x 2 ) sized array (type `CV_64FC1`) of distortion-free keypoint locations in camera coordinates corresponding to `cv::InputArray _points1`
* `double dist`: Threshold on 3D coordinate depth values (z) based on a normalized translation vector (i.e. base length between cameras is 1)

Input/Output variable descriptions:
* `cv::InputOutputArray mask`: Inlier mask of size (rows x cols) = (1 x n) in `cv::Mat` format and type `CV_8UC1`

Output variable descriptions:
* `cv::OutputArray Q3D`: Triangulated 3D coordinates of size (rows x cols) = ( n x 3 ) and type `CV_64FC1` (`cv::Mat`)

#### Bundle Adjustment <a name="interface-pose-ba"></a>

Bundle adjustment on extrinsics (R & t) only or on intrinsics (camera matrices) and extrinsics can be performed using
```
bool poselib::refineStereoBA(cv::InputArray p1,
    cv::InputArray p2,
    cv::InputOutputArray R,
    cv::InputOutputArray t,
    cv::InputOutputArray Q,
    cv::InputOutputArray K1,
    cv::InputOutputArray K2,
    bool pointsInImgCoords = false,
    cv::InputArray mask = cv::noArray(),
    const double angleThresh = 1.25,
    const double t_norm_tresh = 0.05);
```
located within [./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h](./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h).
It can be included with `#include "poselib/pose_estim.h"`.

Bundle adjustment on extrinsics only can be performed by providing `p1` and `p2` in camera coordinates and setting `pointsInImgCoords = false`.
Bundle adjustment on intrinsics and extrinsics can be performed by providing `p1` and `p2` in image coordinates and setting `pointsInImgCoords = true`.
For transforming correspondences from camera into image coordinates function `void CamToImgCoordTrans(cv::Mat& points, cv::Mat K)` located in [./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h](./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h) can be used.

Input variable descriptions:
* `cv::InputArray p1`: (rows x cols) = ( n x 2 ) sized array (type `CV_64FC1`) of distortion-free keypoint locations in camera or image coordinates corresponding to `cv::InputArray p2`
* `cv::InputArray p2`: (rows x cols) = ( n x 2 ) sized array (type `CV_64FC1`) of distortion-free keypoint locations in camera or image coordinates corresponding to `cv::InputArray p1`
* `bool pointsInImgCoords`: Specifies if correspondences `p1` and `p2` are provided in image or camera coordinates
* `cv::InputArray mask`: Inlier mask of size (rows x cols) = (1 x n) in `cv::Mat` format and type `CV_8UC1`
* `double angleThresh`: Threshold angle in degrees. If the difference of input and output rotation is larger this value, it is assumed that bundle adjustment resulted in a wrong local minimum and the initial pose parameters are returned
* `double t_norm_tresh`: Threshold on the translation vector norm: If the difference of input and output translation vector is larger this value, it is assumed that bundle adjustment resulted in a wrong local minimum and the initial pose parameters are returned

Input/Output variable descriptions:
* `cv::InputOutputArray R`: Rotation matrix (`cv::Mat` of type `CV_64FC1`)
* `cv::InputOutputArray t`: Translation vector (`cv::Mat` of type `CV_64FC1`)
* `cv::InputOutputArray Q`: 3D coordinates of size (rows x cols) = ( n x 3 ) and type `CV_64FC1` (`cv::Mat`)
* `cv::InputOutputArray K1`: Camera matrix of first camera (`cv::Mat` of type `CV_64FC1`)
* `cv::InputOutputArray K2`: Camera matrix of second camera (`cv::Mat` of type `CV_64FC1`)

#### Rectification <a name="interface-pose-rectification"></a>

To calculate rectification matrices use
```
int poselib::getRectificationParameters(cv::InputArray R,
    cv::InputArray t,
    cv::InputArray K1,
    cv::InputArray K2,
    cv::InputArray distcoeffs1,
    cv::InputArray distcoeffs2,
    const cv::Size& imageSize,
    cv::OutputArray Rect1,
    cv::OutputArray Rect2,
    cv::OutputArray K1new,
    cv::OutputArray K2new,
    double alpha = -1,
    bool globRectFunct = true,
    const cv::Size& newImgSize = cv::Size(),
    cv::Rect *roi1 = nullptr,
    cv::Rect *roi2 = nullptr,
    cv::OutputArray P1new = cv::noArray(),
    cv::OutputArray P2new = cv::noArray());
```
located within [./matchinglib_poselib/source/poselib/include/poselib/pose_helper.h](./matchinglib_poselib/source/poselib/include/poselib/pose_helper.h).
It can be included with `#include "poselib/pose_helper.h"`.

Input variable descriptions:
* `cv::InputArray R`: Rotation matrix (`cv::Mat` of type `CV_64FC1`)
* `cv::InputArray t`: Translation vector (`cv::Mat` of type `CV_64FC1`)
* `cv::InputArray K1`: Camera matrix of first camera (`cv::Mat` of type `CV_64FC1`)
* `cv::InputArray K2`: Camera matrix of second camera (`cv::Mat` of type `CV_64FC1`)
* `cv::InputArray distcoeffs1`: Radial distortion parameters of first camera correspond to [OpenCV format](https://docs.opencv.org/3.4.9/d9/d0c/group__calib3d.html#ga1019495a2c8d1743ed5cc23fa0daff8c) with 5 or 8 distortion parameters.
* `cv::InputArray distcoeffs2`: Radial distortion parameters of second camera correspond to [OpenCV format](https://docs.opencv.org/3.4.9/d9/d0c/group__calib3d.html#ga1019495a2c8d1743ed5cc23fa0daff8c) with 5 or 8 distortion parameters.
* `cv::Size imageSize`: Size of input images
* `double alpha`: Free scaling parameter. If it is -1 or absent, the function performs the default
scaling. Otherwise, the parameter should be between 0 and 1. `alpha=0` means that the rectified
images are zoomed and shifted so that only valid pixels are visible (no black areas after
rectification). `alpha=1` means that the rectified image is decimated and shifted so that all
the pixels from the original images from the cameras are retained in the rectified images
(no source image pixels are lost). Obviously, any intermediate value yields an intermediate
result between those two extreme cases.
* `bool globRectFunct`: Used method for rectification. If true, the method from A. Fusiello, E. Trucco
and A. Verri: "A compact algorithm for rectification of stereo pairs", 2000 is used. This methode can
be used for the rectification of cameras with a general form of the extrinsic parameters.
If false, a slightly changed version (to be more robust) of the OpenCV stereoRectify() function
is used. This method can be used for stereo cameras with only a small difference in the
vertical position and small rotations only (the cameras should be nearly parallel).
* `cv::Size newImgSize`: Optional new image resolution after rectification. The same size should be passed to
initUndistortRectifyMap() (see the stereo_calib.cpp sample in OpenCV samples directory).
When (0,0) is passed (default), it is set to the original imageSize . Setting it to larger
value can help you preserve details in the original image, especially when there is a big radial distortion.

Output variable descriptions:
* `cv::OutputArray Rect1`: Rectification matrix for the first camera (`cv::Mat` of type `CV_64FC1`)
* `cv::OutputArray Rect2`: Rectification matrix for the second camera (`cv::Mat` of type `CV_64FC1`)
* `cv::OutputArray K1new`: New camera matrix for the first camera (`cv::Mat` of type `CV_64FC1`)
* `cv::OutputArray K2new`: New camera matrix for the second camera (`cv::Mat` of type `CV_64FC1`)
* `cv::Rect *roi1`: Optional output rectangles inside the first rectified image where all the pixels are valid.
If `alpha=0`, the ROIs cover the whole images. Otherwise, they are likely to be smaller.
* `cv::Rect *roi2`: Optional output rectangles inside the second rectified image where all the pixels are valid.
If `alpha=0`, the ROIs cover the whole images. Otherwise, they are likely to be smaller.
* `cv::OutputArray P1new`: Optional new projection matrix (`cv::Mat` of type `CV_64FC1`) for the first camera (only available if `globRectFunct=true`)
* `cv::OutputArray P2new`: Optional new projection matrix (`cv::Mat` of type `CV_64FC1`) for the second camera (only available if `globRectFunct=true`)

After calculating rectification parameters, OpenCV's [initUndistortRectifyMap](https://docs.opencv.org/4.2.0/d9/d0c/group__calib3d.html#ga7dfb72c9cf9780a347fbe3d1c47e5d5a) function for calculating rectification maps and [remap()](https://docs.opencv.org/3.4.9/da/d54/group__imgproc__transform.html#gab75ef31ce5cdfb5c44b6da5f3b908ea4) for remapping source images can be used.

#### Additional information <a name="interface-pose-example"></a>

Additional helpful functions can be found in header files:
* [./matchinglib_poselib/source/poselib/include/poselib/pose_helper.h](./matchinglib_poselib/source/poselib/include/poselib/pose_helper.h)
* [./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h](./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h)

An example can be found in [./matchinglib_poselib/source/tests/poselib-test/main.cpp](./matchinglib_poselib/source/tests/poselib-test/main.cpp).

### Continuous High Accuracy Stereo Pose Estimation <a name="interface-stereo"></a>

Using this functionality allows to estimate and continuously refine relative camera poses while detecting pose changes to achieve accuracies compareable to offline camera calibration.
All necessary functions and classes can be found in header file [./matchinglib_poselib/source/poselib/include/poselib/stereo_pose_refinement.h](./matchinglib_poselib/source/poselib/include/poselib/stereo_pose_refinement.h).
It can be included with `#include "poselib/stereo_pose_refinement.h"`.

First, create an object of class `StereoRefine` and provide `struct poselib::ConfigPoseEstimation`.
A description of each parameter can be found in [./matchinglib_poselib/source/poselib/include/poselib/stereo_pose_refinement.h](./matchinglib_poselib/source/poselib/include/poselib/stereo_pose_refinement.h).

For each available stereo image pair call member function
```
int poselib::StereoRefine::addNewCorrespondences(std::vector<cv::DMatch> matches,
    std::vector<cv::KeyPoint> kp1,
    std::vector<cv::KeyPoint> kp2,
    const poselib::ConfigUSAC &cfg)
```

Input variable descriptions:
* `std::vector<cv::DMatch> matches`: Matches in [OpenCV format](https://docs.opencv.org/4.2.0/d4/de0/classcv_1_1DMatch.html)
* `std::vector<cv::KeyPoint> kp1`: Keypoints in first image in [OpenCV format](https://docs.opencv.org/4.2.0/d2/d29/classcv_1_1KeyPoint.html)
* `std::vector<cv::KeyPoint> kp2`: Keypoints in second image in [OpenCV format](https://docs.opencv.org/4.2.0/d2/d29/classcv_1_1KeyPoint.html)
* `poselib::ConfigUSAC cfg`: Configuration parameters for USAC. See [./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h](./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h).

Estimated or refined poses can be accessed with member variables
* `poselib::StereoRefine::R_new`
* `poselib::StereoRefine::t_new`

The software estimates after a few successful pose estimations/refinements a most likely pose based on the history of pose calculations in a Monte-Carlo similar fashion.
This pose can be accessed with member variables:
* `poselib::StereoRefine::R_mostLikely`
* `poselib::StereoRefine::t_mostLikely`

If this "most likely" pose is found to be stable, the flag `poselib::StereoRefine::mostLikelyPose_stable` is set to true.

Stability on refined poses `poselib::StereoRefine::R_new` and `poselib::StereoRefine::t_new` is signaled by flag `poselib::StereoRefine::poseIsStable`.

An example can be found in [./matchinglib_poselib/source/tests/poselib-test/main.cpp](./matchinglib_poselib/source/tests/poselib-test/main.cpp).

### ROS <a name="interface-ros"></a>

We provide a ROS interface for this library including all functionalities described above ([matching](#executable-matching), [pose estimation](#executable-pose), and [continuous high accuracy stereo pose estimation](#interface-stereo)).
The interface continuously reads image data and calculates matches and poses.
Parameters can be set by providing a launch file and/or dynamically reconfiguring parameters during runtime.

The ROS interface and instructions on how to set-up and use it can be found [here](https://github.com/josefmaierfl/matchinglib_poselib_ros).

## Testing Results on Supported Keypoint, Descriptor, and Matching Algorithm Types <a name="tests-features"></a>

Performance evaluations on keypoint-descriptor combinations can be found in
* [J. Maier et.al., Ground Truth Accuracy and Performance of the Matching Pipeline, CVPRW, 2017](https://openaccess.thecvf.com/content_cvpr_2017_workshops/w10/html/Maier_Ground_Truth_Accuracy_CVPR_2017_paper.html)
* [Descriptor performance on AKAZE detector](./PDF_evaluations_on_keypoints_descriptors_matching/Descriptor_performance_figures_using_AKAZE_detector.pdf)
* [Descriptor performance on BRISK detector](./PDF_evaluations_on_keypoints_descriptors_matching/Descriptor_performance_figures_using_BRISK_detector.pdf)
* [Descriptor performance on FAST detector](./PDF_evaluations_on_keypoints_descriptors_matching/Descriptor_performance_figures_using_FAST_detector.pdf)
* [Descriptor performance on KAZE detector](./PDF_evaluations_on_keypoints_descriptors_matching/Descriptor_performance_figures_using_KAZE_detector.pdf)
* [Descriptor performance on MSD detector](./PDF_evaluations_on_keypoints_descriptors_matching/Descriptor_performance_figures_using_MSD_detector.pdf)
* [Descriptor performance on ORB detector](./PDF_evaluations_on_keypoints_descriptors_matching/Descriptor_performance_figures_using_ORB_detector.pdf)
* [Descriptor performance on SIFT detector](./PDF_evaluations_on_keypoints_descriptors_matching/Descriptor_performance_figures_using_SIFT_detector.pdf)
* [Performance metrics of keypoint-descriptor combinations](./PDF_evaluations_on_keypoints_descriptors_matching/Performance_metrics_Keypoint-Descriptor-Combinations.pdf)
* [Evaluations on runtime of descriptors](./PDF_evaluations_on_keypoints_descriptors_matching/Runtime_Descriptors.pdf)

Performance evaluations on matching algorithms can be found in
* [J. Maier et.al., Guided Matching Based on Statistical Optical Flow for Fast and Robust Correspondence Analysis, ECCV, 2016](https://link.springer.com/chapter/10.1007/978-3-319-46478-7_7)
* [Supplementary material](https://link.springer.com/chapter/10.1007/978-3-319-46478-7_7#SupplementaryMaterial)
* [Matching algorithm performance figures](./PDF_evaluations_on_keypoints_descriptors_matching/Matching_algorithms_performance_figures.pdf)
* [Evaluations on runtime of matching algorithms](./PDF_evaluations_on_keypoints_descriptors_matching/Runtime_Matching_algorithms.pdf)

## Testing Results on Supported Pose Estimation Algorithms <a name="tests-pose"></a>

Testing results can be found [here](https://github.com/josefmaierfl/autocalib_test_package).

## Publication <a name="publication"></a>

Please cite the following papers if you use this library or parts of this code in your own work.

```
@inproceedings{maier2016guided,
  title={Guided Matching Based on Statistical Optical Flow for Fast and Robust Correspondence Analysis},
  author={Maier, Josef and Humenberger, Martin and Murschitz, Markus and Zendel, Oliver and Vincze, Markus},
  booktitle={European Conference on Computer Vision},
  pages={101--117},
  year={2016},
  organization={Springer}
}
```

```
@inproceedings{maier2017ground,
  title={Ground truth accuracy and performance of the matching pipeline},
  author={Maier, Josef and Humenberger, Martin and Zendel, Oliver and Vincze, Markus},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  pages={29--39},
  year={2017}
}
```
