# MATCHING- AND POSELIB

- [Introduction](#introduction)
- [Supported Keypoint and Descriptor Types](#support-features)
- [Supported Matching Algorithms](#support-matching)
- [Supported Correspondence Filtering Techniques](#support-filtering)
- [Supported Pose Estimation Algorithms](#support-pose)
- [Installation](#installation)
    - [Dependencies](#dependencies)
        - [Docker](#docker)
        - [System-wide Installation on Linux Systems](#system-dependencies)
    - [Using Stand-Alone Executables without Docker](#executable)
    - [Library](#library)
- [Quick Start: Using Provided Test Data](#quick-start)
    - [Sparse Feature Matching](#quick-matching)
    - [Pose Estimation](#quick-pose)
- [Stand-Alone Executable for Feature Matching](#executable-matching)
- [Stand-Alone Executable for Pose Estimation](#executable-pose)
- [Stand-Alone Executable for Reading Test Data Generated by SemiRealSequence](#executable-test)
- [Interfacing the Library](#interface-lib)
    - [Calculation of Sparse Feature Matches](#interface-matching)
    - [Calculation of Relative Poses](#interface-pose)
    - [Continuous High Accuracy Stereo Pose Estimation](#interface-stereo)
    - [ROS](#interface-ros)
- [Testing Results on Supported Keypoint, Descriptor, and Matching Algorithm Types](#tests-features)
- [Testing Results on Supported Pose Estimation Algorithms](#tests-pose)
- [Publication](#publication)

## Introduction <a name="introduction"></a>

This library includes various algorithms for calculating, filtering, refining, and matching of sparse image features.
Moreover, multiple algorithms for estimating relative poses like different random sample consensus algorithms, 5pt solvers, cost functions, linear pose refinement algorithms, bundle adjustment (BA) options, and stereo rectification algorithms are provided.
For continuously estimating high accurate relative stereo poses, the library includes a framework that estimates stereo poses and continuously refines them using multiple stereo images (e.g. from a streaming stereo rig) while detecting pose changes to achieve pose accuracies comparable to offline calibration methods.

For easy interfacing, we provide 3 possibilities to interface the library:
* Stand-alone executables with a command-line interface to read image and calibration data from disk
* A library that can be integrated into your own application
* A ROS interface for reading continuous image data providing a launch file and the possibility to dynamically reconfigure parameters during runtime

We further provide testing results on various keypoint-descriptor combinations, matching algorithms, and pose estimation algorithms.

##Supported Keypoint and Descriptor Types <a name="support-features"></a>

Currently, all keypoint and descriptor types within [OpenCV](https://docs.opencv.org/4.2.0/d5/d51/group__features2d__main.html) (including [contrib](https://docs.opencv.org/4.2.0/d7/d7a/group__xfeatures2d__experiment.html)) in addition to [BOLD](https://github.com/vbalnt/bold) and [RIFF](http://press.liacs.nl/publications/RIFF%20-%20Retina-inspired%20Invariant%20Fast%20Feature%20Descriptor.pdf) are supported.

Keypoint types:
* FAST
* MSER
* ORB
* BRISK
* KAZE
* AKAZE
* STAR
* MSD
* SIFT
* SURF

Descriptor types:
* BRISK
* ORB
* KAZE
* AKAZE
* FREAK
* DAISY
* LATCH
* BGM
* BGM_HARD
* BGM_BILINEAR
* LBGM
* BINBOOST_64
* BINBOOST_128
* BINBOOST_256
* VGG_120
* VGG_80
* VGG_64
* VGG_48
* SIFT
* SURF
* RIFF
* BOLD

To enable SIFT and SURF, OpenCV must be built with enabled non-free code (contrib) and option `-DUSE_NON_FREE_CODE=ON` must be provided to CMake when building this library.

## Supported Matching Algorithms <a name="support-matching"></a>

* CASHASH:	    Cascade Hashing matcher from the [NMSLIB](https://github.com/nmslib/nmslib)
* GMBSOF:	    [Guided Matching based on Statistical Optical Flow](https://link.springer.com/chapter/10.1007/978-3-319-46478-7_7)
* HIRCLUIDX:    Hirarchical Clustering Index Matching from the [FLANN library](https://github.com/mariusmuja/flann)
* HIRKMEANS:    Hierarchical k-means tree matcher from the [FLANN library](https://github.com/mariusmuja/flann)
* LINEAR:	    Linear matching algorithm (Brute force) from the [FLANN library](https://github.com/mariusmuja/flann)
* LSHIDX:	    LSH Index Matching algorithm from the [FLANN library](https://github.com/mariusmuja/flann)
* RANDKDTREE:	Randomized KD-trees matcher from the [FLANN library](https://github.com/mariusmuja/flann)
* SWGRAPH:	    Small World Graph (SW-graph) from the [NMSLIB](https://github.com/nmslib/nmslib)
* HNSW:         Hiarchical Navigable Small World Graph from the [NMSLIB](https://github.com/nmslib/nmslib)
* VPTREE:       VP-tree or ball-tree from the [NMSLIB](https://github.com/nmslib/nmslib)
* MVPTREE:      Multi-Vantage Point Tree from the [NMSLIB](https://github.com/nmslib/nmslib)
* GHTREE:       GH-Tree from the [NMSLIB](https://github.com/nmslib/nmslib)
* LISTCLU:      List of clusters from the [NMSLIB](https://github.com/nmslib/nmslib)
* SATREE:       Spatial Approximation Tree from the [NMSLIB](https://github.com/nmslib/nmslib).
* BRUTEFORCENMS: Brute-force (sequential) searching from the [NMSLIB](https://github.com/nmslib/nmslib)
* ANNOY:        [Approximate Nearest Neighbors Matcher](https://github.com/spotify/annoy)
* LKOF:         Lucas Kanade Optical Flow
* LKOFT:        Lucas Kanade Optical Flow Tracker
* ALKOF:        Advanced Lucas Kanade Optical Flow
* ALKOFT:       Advanced Lucas Kanade Optical Flow Tracker

## Supported Correspondence Filtering Techniques <a name="support-filtering"></a>

After matching, we provide possibilities to filter found matches:
* Descriptor distance ratio filter (Lowe's ratio test)
* GMS: [Grid-based Motion Statistics for Fast, Ultra-robust Feature Correspondence](https://github.com/JiawangBian/GMS-Feature-Matcher)
* VFC: [Vector Field Consensus](https://github.com/jiayi-ma/VFC)
* SOF: [Statistical Optical Flow](https://link.springer.com/chapter/10.1007/978-3-319-46478-7_7)

## Supported Pose Estimation Algorithms <a name="support-pose"></a>

This library supports multiple algorithms and combinations thereof to estimate relative poses between cameras.
We included several ***random sample consensus algorithms***:
* RANSAC
* [USAC](https://ieeexplore.ieee.org/document/6365642) (adapted to estimate PROSAC beta and SPRT delta)
* [ARRSAC](https://github.com/rust-cv/arrsac)
* [MLESAC](http://www.robots.ox.ac.uk/~vgg/publications/papers/torr00.pdf)
* [NG-RANSAC](https://github.com/vislearn/ngransac) (not included in this repository but integrated in the [testing-version of this library](https://github.com/josefmaierfl/autocalib_test_package/tree/ngransac))

USAC additionally supports the ***detection and correction of degenerate cases*** (i.e. rotation only) using [QDEGSAC](https://people.inf.ethz.ch/pomarc/pubs/QDEGSAC.pdf) or a USAC-internal solution.

The following algorithms for ***pose estimation*** can be used within RANSAC variants:
* 5pt solvers (some of them from [OpenGV](http://laurentkneip.github.io/opengv/))
    * [Nister](http://www.ee.oulu.fi/research/imag/courses/Sturm/nister04.pdf)
    * [Stewenius](http://www.robots.ox.ac.uk/~vgg/publications/papers/stewenius05a.pdf)
    * [Kneip](http://laurentkneip.github.io/opengv/)
* [8pt algorithm](http://www.cs.cmu.edu/afs/andrew/scs/cs/15-463/f07/proj_final/www/amichals/fundamental.pdf)
* Homography alignment

For each of the afore mentioned algorithms different ***cost functions*** can be chosen:
* Squared norm
* Torr weights ([Torr dissertation](http://www.robots.ox.ac.uk/~phst/Papers/SPIE93/m.ps.gz), Eqn. 2.25)
* [Pseudo-Huber](https://en.wikipedia.org/wiki/Huber_loss)

Robustly estimated minimal sample models (i.e. Essential matrix or rotation matrix and translation vector) can be subsequently ***refined*** utilizing all inliers by a combination of above mentioned solvers and cost functions.

In addition, models can be refined by applying ***bundle adjustment (BA)*** on extrinsics only or on both, extrinsics and intrinsics.

We further implemented a ***linear rectification algorithm*** for general, unconstrained stereo rigs based on the publication ["A compact algorithm for rectification of stereo pairs"](https://link.springer.com/article/10.1007/s001380050120).

All above mentioned methods, algorithms, and cost functions can further be used within a ***continious high accurate relative stereo pose estimation framework*** which estimates a relative stereo pose and refines it utilizing additional stereo image pairs by concatenating the "best" correspondences over multiple stereo frames while detecting pose changes.

## Installation <a name="installation"></a>

The library was tested on Ubuntu and Windows but we provide a [Docker](https://docs.docker.com/get-docker/) file for operating system independent usage.
We provide multiple possibilities to use this software:
* Stand-alone executables to
    * Calculate image features and perform matching based on images stored on disk
    * Calculate relative poses (mono and stereo cameras) based on images and calibration (camera intrinsics) info (based on [KITTI format](https://s3.eu-central-1.amazonaws.com/avg-kitti/devkit_raw_data.zip)) stored on disk
    * Test algorithms based on GT data generated by [SemiRealSequence](https://github.com/josefmaierfl/SemiRealSequence)
* [Docker](https://docs.docker.com/get-docker/) image with afore mentioned stand-alone executables
* [ROS interface](https://github.com/josefmaierfl/matchinglib_poselib_ros)
* Library for integration into your own application

### Dependencies <a name="dependencies"></a>

#### Docker <a name="docker"></a>

If you only want to use the provided executables, [Docker](https://docs.docker.com/get-docker/) can be used.
After installing Docker, the corresponding Docker image can be built by executing `./build_docker_base.sh` in the main directory of this repository.
On Windows, the image can be built by executing `docker build -t poselib:1.0 .` in the main directory of this repository using Powershell.

#### System-wide Installation on Linux Systems <a name="system-dependencies"></a>

SemiRealSequence depends on the following libraries:
* [Eigen 3.3.7](http://eigen.tuxfamily.org/index.php?title=Main_Page)
* [Boost](https://www.boost.org/)
* [OpenCV 4.2.0](https://opencv.org/)
* [SBA](https://github.com/balintfodor/sba)
* [CLAPACK](https://github.com/NIRALUser/CLAPACK)

For installing above libraries, the following packages should be installed:
```bash
sudo apt-get update
sudo apt-get install libboost-all-dev
sudo apt-get install build-essential cmake pkg-config
sudo apt-get install wget \
    libtbb2 \
    libtbb-dev \
    libglew-dev \
    qt5-default \
    libxkbcommon-dev \
    libflann-dev \
    libpng-dev \
    libgtk-3-dev \
    libgtkglext1 \
    libgtkglext1-dev \
    libtiff-dev \
    libtiff5-dev \
    libtiffxx5 \
    libjpeg-dev \
    libjasper1 \
    libjasper-dev \
    libavcodec-dev \
    libavformat-dev \
    libswscale-dev \
    libv4l-dev \
    libxvidcore-dev \
    libx264-dev \
    libdc1394-22-dev \
    openexr \
    libatlas-base-dev \
    gfortran
sudo apt-get install libglu1-mesa-dev mesa-common-dev mesa-utils freeglut3-dev
sudo apt-get install libomp-dev
```
All mentioned libries (OpenCV, ...) can be installed by executing `./build_thirdparty.sh` within directory `ci` of this repository.
If some of the libraries are already installed on your system, missing libraries can be installed using the corresponding script file within directory [ci](./ci).
CLAPACK and SBA can be installed by executing commands below within directory [ci](./ci):
```bash
thirdparty_dir="$(pwd)/thirdparty"
cd ${thirdparty_dir}/clapack-3.2.1/build/generic
./build.sh
cd ${thirdparty_dir}/sba-1.6/build/generic
./build.sh
```

Libraries integrated into this library (no need for installation):
* [FLANN library](https://github.com/mariusmuja/flann)
* [NMSLIB](https://github.com/nmslib/nmslib)
* [ANNOY](https://github.com/spotify/annoy)
* [nanoflann](https://github.com/jlblancoc/nanoflann)
* [GMS](https://github.com/JiawangBian/GMS-Feature-Matcher)
* [VFC](https://github.com/jiayi-ma/VFC)
* [BOLD](https://github.com/vbalnt/bold)
* [RIFF](https://press.liacs.nl/researchdownloads/riff/Descriptor_Project(mm2014).zip)
* [ARRSAC](https://github.com/rust-cv/arrsac)
* [USAC](http://www.cs.unc.edu/~rraguram/usac/)
* [OpenGV](http://laurentkneip.github.io/opengv/)

### Using Stand-Alone Executables without Docker <a name="executable"></a>

After [installing dependencies](#system-dependencies), the library and executables can be built by executing `./build_install_matchinglib_poselib.sh exe` (no system-wide installation) or by (within main repository directory)
```bash
cd matchinglib_poselib
mkdir build
cd build
cmake .. -DCMAKE_BUILD_TYPE=Release -DOPTION_BUILD_TESTS=ON
make -j "$(nproc)"
```
Executables for matching features `matchinglib-test`, pose estimation `poselib-test`, and testing `noMatch_poselib-test` are located in the build folder.

### Library <a name="library"></a>

To use the library within your own application, install all necessary [dependencies](#system-dependencies) and subsequently build and install it by calling

`./build_install_matchinglib_poselib.sh install`

or by performing the following steps:

```bash
cd matchinglib_poselib
mkdir build
cd build
cmake .. -DCMAKE_BUILD_TYPE=Release -DOPTION_BUILD_TESTS=OFF -DBUILD_SHARED_LIBS=ON
make -j "$(nproc)"
sudo make install
```

To integrate it within CMake include the following into your `CMakeLists.txt`:

```
find_package(matchinglib_poselib REQUIRED)
find_package(Eigen REQUIRED)
find_package(OpenCV 4.2.0 REQUIRED)

target_link_libraries(your_project_name
  ${OpenCV_LIBS}
  sba
  ${CLAPACK_LIBRARIES}
  matchinglib_poselib::matchinglib
  matchinglib_poselib::poselib)

target_include_directories(your_project_name
  PRIVATE
  ${DEFAULT_INCLUDE_DIRECTORIES}
  ${OpenCV_INCLUDE_DIRS}
  ${Eigen_INCLUDE_DIR}
  ${CLAPACK_INCLUDE_DIRS}
  matchinglib_poselib::matchinglib
  matchinglib_poselib::poselib)
```

## Quick Start: Using Provided Test Data <a name="quick-start"></a>

This repository includes a small test data set (images and intrinsics calibration information) which is copied into the build directory during the CMake build process.
This can be disabled by providing options `-DCOPY_TEST_MATCH_IMGS=OFF` and/or `COPY_TEST_POSE_IMGS=OFF` to CMake.

### Sparse Feature Matching <a name="quick-matching"></a>

To start extracting features and calculating matches utilizing some default parameters call `./matchinglib-test` within your build directory or, if you are using Docker, run `./run_docker_base.sh match` within the main directory of this repository.
This will show a subset of found matches for every image pair.

To change parameters and/or the directory containing images execute `./matchinglib-test -h` or `./run_docker_base.sh match -h` to show options.

### Pose Estimation <a name="quick-pose"></a>

To start extracting features, calculating matches, and estimating poses utilizing some default parameters call `./poselib-test` within your build directory or, if you are using Docker, run `./run_docker_base.sh pose` within the main directory of this repository.
This will show a subset of found matches and rectified images for every input image pair.
The accuracy of estimated poses and rectification can be checked by moving your mouse cursor on the rectified images.

To change parameters and/or the directory containing images execute `./poselib-test -h` or `./run_docker_base.sh pose -h` to show options.

## Stand-Alone Executable for Feature Matching <a name="executable-matching"></a>

For testing executbale `matchinglib-test` with default parameters and the provided test images see [here](#quick-matching).

Feature matches can be calculated using all available algorithms (see [here](#support-features), [here](#support-matching), and [here](#support-filtering)).
Algorithm names can be specified as strings like listed [here](#support-features) and [here](#support-matching) with command line options `--f_detect`, `--d_extr`, and `--matcher`.

Feature matches can be calculated for multiple mono and stereo camera images located within a single folder by either providing only a pre- and/or post-fix using `--l_img_pref` for one image sequence or by providing 2 pre- and/or post-fixes using `--l_img_pref` and `--r_img_pref` for stereo image sequences, respectively.
By default, matches are not stored but displayed.
To store keypoints and matches provide option `--output_path`.
To disable displaying found matches provide option `--showNr -3`.

To start calculating features and matches call `./matchinglib-test [options]` or `./run_docker_base.sh match [options]`.

For additional options and details call `./matchinglib-test -h` or `./run_docker_base.sh match -h`.

## Stand-Alone Executable for Pose Estimation <a name="executable-pose"></a>

For testing executbale `poselib-test` with default parameters and the provided test data see [here](#quick-pose).

Executable `poselib-test` supports the calculation of feature matches with identical options as for executbale `matchinglib-test` (see [here](#executable-matching)) with subsequent relative pose estimation.
All available algorithms described/listed [here](#support-pose) can be used.
For details call `./poselib-test -h` or `./run_docker_base.sh pose -h`.

Realtive poses can be calculated for multiple mono and stereo camera images located within a single folder by either providing only a pre- and/or post-fix using `--l_img_pref` for one image sequence or by providing 2 pre- and/or post-fixes using `--l_img_pref` and `--r_img_pref` for stereo image sequences, respectively.
For mono camera configurations (frame-to-frame poses and rectified images are calculated), a step size providing option `--stepSize` can be specified.
In addition, intrinsics calibration data (or at least an estimate with enabled bundle adjustment) must be provided.
This executable only supports reading intrinsics data for at most 2 cameras (mono, stereo).
Intrinsics data must be provided with a text file within the provided image directory.
The data format corresponds to [raw KITTI calibration format](https://s3.eu-central-1.amazonaws.com/avg-kitti/devkit_raw_data.zip).
The text file must also include extrinsics data which can be set to zero translation and identity rotation matrix.
Extrinsics data is only used for comparison (which can be disabled) with estimated poses.

To store rectified images provide option `--output_path`.
To disable displaying found matches, informational output, and rectified images provide options `--v 0 --showNr -3`.

To enable the continious high accurate relative stereo pose estimation which calculates poses based on multiple stereo frames that have identical intrinsics and extrinsics for at least a few frames, provide option `--stereoRef`.

To start calculating features, matches and relative poses call `./poselib-test [options]` or `./run_docker_base.sh pose [options]`.

For additional options and details call `./poselib-test -h` or `./run_docker_base.sh pose -h`.

## Stand-Alone Executable for Reading Test Data Generated by SemiRealSequence <a name="executable-test"></a>

Executable `noMatch_poselib-test` can be used for evaluation of all pose estimation related (excluding algorithms for calculating image features and matches) algorithms within this library.
Parameters are basically equal to executable `poselib-test` without parameters for feature calculation and matching.

Input are 3D data and correspondences generated by [SemiRealSequence](https://github.com/josefmaierfl/SemiRealSequence) which allows testing with GT data and for different 3D and correspondence data properties.
This enables finding optimal algorithm and parameter combinations for specific or general scene, pose, and camera properties.

We performed testing on over a million test scenarios and algorithm configurations using [SemiRealSequence](https://github.com/josefmaierfl/SemiRealSequence) and this library.
The full test framework can be found [here](https://github.com/josefmaierfl/autocalib_test_package).
Additional tests using [NG-RANSAC](https://github.com/vislearn/ngransac) which was integrated into an adapted version of executable `poselib-test` were performed using a [slightly adapted test framework](https://github.com/josefmaierfl/autocalib_test_package/tree/ngransac) to support [PyTorch](https://pytorch.org/).
For testing results, see [here](#tests-pose).

Sequences generated by [SemiRealSequence](https://github.com/josefmaierfl/SemiRealSequence) can be loaded by providing the path to generated 3D data with option `--sequ_path` in addition to a number (e.g. 0, 1, 2, 3, ...) of sequentially generated correspondence data with different properties for the same 3D data.
If e.g. only one set of correspondence data was generated, option `--matchData_idx 0` must be provided.
Moreover, option `--ovf_ext` which specifies the data format and file extension (Possible options: yaml/xml/yaml.gz/xml.gz) of data to read must be provided.
Results of performed tests are stored to disk in CSV format within the directory provided by option `--output_path`.

Information on how to use [SemiRealSequence](https://github.com/josefmaierfl/SemiRealSequence) can be found in the corresponding [repository](https://github.com/josefmaierfl/SemiRealSequence).

## Interfacing the Library <a name="interface-lib"></a>

The library is split into 2 main parts: `matchinglib` and `poselib`.
To use them, install all [required dependencies](#system-dependencies) and [the library itself](#library) and [adopt your CMake](#library).

### Calculation of Sparse Feature Matches <a name="interface-matching"></a>

After loading 2 grayscale images `img1` and `img2` in OpenCV format (`cv::Mat`), matches can be calculated by calling function
```
int matchinglib::getCorrespondences(cv::Mat& img1,
                       cv::Mat& img2,
                       std::vector<cv::DMatch> & finalMatches,
                       std::vector<cv::KeyPoint> & kp1,
                       std::vector<cv::KeyPoint> & kp2,
                       std::string featuretype = "FAST",
                       std::string extractortype = "FREAK",
                       std::string matchertype = "GMBSOF",
                       bool dynamicKeypDet = true,
                       int limitNrfeatures = 8000,
                       bool VFCrefine = false,
                       bool GMSrefine = false,
                       bool ratioTest = true,
                       bool SOFrefine = false,
                       int subPixRefine = 0,
                       int verbose = 0,
                       std::string idxPars_NMSLIB = "",
                       std::string queryPars_NMSLIB = "");
```
located in [./matchinglib_poselib/source/matchinglib/include/matchinglib/matchinglib_correspondences.h](./matchinglib_poselib/source/matchinglib/include/matchinglib/matchinglib_correspondences.h).
Include it with `#include "matchinglib/matchinglib.h"`.

Input variable descriptions:
* `cv::Mat img1`: First loaded grayscale image
* `cv::Mat img2`: Second loaded grayscale image
* `std::string featuretype`: Keypoint type as listed [here](#support-features)
* `std::string extractortype`: Descriptor type as listed [here](#support-features)
* `std::string matchertype`: Matching algorithm to use as listed [here](#support-matching)
* `bool dynamicKeypDet`: If true, found keypoints are filtered by local keypoint responses (i.e. [cornerness and blob strength](https://docs.opencv.org/4.2.0/d2/d29/classcv_1_1KeyPoint.html#a1f163ac418c281042e28895b20514360)) using an image grid to limit the number of keypoints to `limitNrfeatures`.
* `int limitNrfeatures`: Limit on the number keypoints for each image
* `bool VFCrefine`: Use the [VFC](https://github.com/jiayi-ma/VFC) algorithm to filter keypoints
* `bool GMSrefine`: Use the [GMS](https://github.com/JiawangBian/GMS-Feature-Matcher) algorithm to filter keypoints
* `bool ratioTest`: Use Lowe's ratio test
* `bool SOFrefine`: Use the [SOF](https://link.springer.com/chapter/10.1007/978-3-319-46478-7_7) algorithm to filter keypoints
* `int subPixRefine`: Use local patch matching to refine found keypoint locations
* `int verbose`: Verbosity level
* `std::string idxPars_NMSLIB`: Optional parameters for matching algorithms of the integrated [NMSLIB](https://github.com/nmslib/nmslib). See their documentation for details.
* `std::string queryPars_NMSLIB`: Optional parameters for matching algorithms of the integrated [NMSLIB](https://github.com/nmslib/nmslib). See their documentation for details.

Output variable descriptions:
* `std::vector<cv::DMatch> finalMatches`: Found matches in [OpenCV format](https://docs.opencv.org/4.2.0/d4/de0/classcv_1_1DMatch.html)
* `std::vector<cv::KeyPoint> kp1`: Found (and optionally filtered) keypoints in first image in [OpenCV format](https://docs.opencv.org/4.2.0/d2/d29/classcv_1_1KeyPoint.html)
* `std::vector<cv::KeyPoint> kp2`: Found (and optionally filtered) keypoints in second image in [OpenCV format](https://docs.opencv.org/4.2.0/d2/d29/classcv_1_1KeyPoint.html)

An example can be found in [./matchinglib_poselib/source/tests/matchinglib-test/main.cpp](./matchinglib_poselib/source/tests/matchinglib-test/main.cpp).

### Calculation of Relative Poses <a name="interface-pose"></a>

#### Preliminaries <a name="interface-pose-preliminaries"></a>

Using matches (e.g. calculated using function `matchinglib::getCorrespondences`), the follwing steps should be performed:
```
//Extract coordinates from keypoints
std::vector<cv::Point2f> points1, points2;
for (auto &j : finalMatches)
{
    points1.push_back(kp1[j.queryIdx].pt);
    points2.push_back(kp2[j.trainIdx].pt);
}

//Transfer into camera coordinates
poselib::ImgToCamCoordTrans(points1, K0);
poselib::ImgToCamCoordTrans(points2, K1);

//Undistort
poselib::Remove_LensDist(points1, points2, dist0_8, dist1_8);
```

Function `poselib::ImgToCamCoordTrans(std::vector<cv::Point2f>& points, cv::Mat K)` transfers matching keypoint locations `points1` and `points2` from the image into the camera coordinate system using camera matrices of first and second cameras, `cv::Mat K0` and `cv::Mat K1`, respectively.
Function `poselib::ImgToCamCoordTrans` can be found in [./matchinglib_poselib/source/poselib/include/poselib/pose_helper.h](./matchinglib_poselib/source/poselib/include/poselib/pose_helper.h) and can be included with `#include "poselib/pose_helper.h"`.
Function `poselib::Remove_LensDist(std::vector<cv::Point2f>& points1, std::vector<cv::Point2f>& points2, const cv::Mat& dist1, const cv::Mat& dist2)` can be used to remove radial distortion and is located in `pose_helper.h`.
Radial distortion parameters `dist1` and `dist2` correspond to [OpenCV format](https://docs.opencv.org/3.4.9/d9/d0c/group__calib3d.html#ga1019495a2c8d1743ed5cc23fa0daff8c) with 5 or 8 distortion parameters.

Based on distortion-free correspondences in camera coordinates, an Essential matrix can be estimated by one of [above listed robust methods](#support-pose).

#### USAC <a name="interface-pose-usac"></a>

In case you want to use [USAC](http://www.cs.unc.edu/~rraguram/usac/) call function
```
int estimateEssentialOrPoseUSAC(const cv::Mat & p1,
	const cv::Mat & p2,
	cv::OutputArray E,
	double th,
	ConfigUSAC & cfg,
	bool & isDegenerate,
	cv::OutputArray inliers = cv::noArray(),
	cv::OutputArray R_degenerate = cv::noArray(),
	cv::OutputArray inliers_degenerate_R = cv::noArray(),
	cv::OutputArray R = cv::noArray(),
	cv::OutputArray t = cv::noArray(),
	bool verbose = false)
```
located within [./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h](./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h).
It can be included with `#include "poselib/pose_estim.h"`.

Input variable descriptions:
* `cv::Mat p1`: (rows x cols) = ( n x 2 ) sized array (type `CV_64FC1`) of distortion-free keypoint locations in camera coordinates corresponding to `cv::Mat p2`
* `cv::Mat p2`: (rows x cols) = ( n x 2 ) sized array (type `CV_64FC1`) of distortion-free keypoint locations in camera coordinates corresponding to `cv::Mat p1`
* `double th`: Threshold in camera coordinates. It can be converted from image coordinates using <img src="https://render.githubusercontent.com/render/math?math=t\!h_{cam} = t\!h_{pix}\frac{4}{\sqrt{2}\left(f_{1,x} %2B f_{1,y} %2B f_{2,x} %2B f_{2,y}\right)}"> with focal lengths *f* of first and second camera in x- and y- direction
* `ConfigUSAC cfg`: USAC configuration parameters. See [./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h](./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h)
* `bool verbose`: Verbosity

Output variable descriptions:
* `cv::OutputArray E`: Essential matrix in `cv::Mat` format and type `CV_64FC1`
* `bool isDegenerate`: Returns true if solution is degenerate (i.e. rotation only)
* `cv::OutputArray inliers`: Inlier mask of size (rows x cols) = (1 x n) in `cv::Mat` format and type `CV_8UC1`
* `cv::OutputArray R_degenerate`: Rotation matrix if `isDegenerate=true`
* `cv::OutputArray inliers_degenerate_R`: Inlier mask (`cv::Mat` format and type `CV_8UC1`) for degenerate solution if `isDegenerate=true`
* `cv::OutputArray R`: Rotation matrix in case Kneip's Eigen solver was used in `cv::Mat` format and type `CV_64FC1`
* `cv::OutputArray t`: Translation vector of size (rows x cols) = (3 x 1) in `cv::Mat` format and type `CV_64FC1`

#### RANSAC, ARRSAC, and MLESAC <a name="interface-pose-robust"></a>

In case you want to use vanilla RANSAC, ARRSAC, or MLESAC call function
```
bool poselib::estimateEssentialMat(cv::OutputArray E,
        cv::InputArray p1,
        cv::InputArray p2,
        const std::string &method,
        double threshold,
        bool refine,
        cv::OutputArray mask = cv::noArray());
```
located within [./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h](./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h).
It can be included with `#include "poselib/pose_estim.h"`.

Input variable descriptions:
* `cv::InputArray p1`: (rows x cols) = ( n x 2 ) sized array (type `CV_64FC1`) of distortion-free keypoint locations in camera coordinates corresponding to `cv::InputArray p2`
* `cv::InputArray p2`: (rows x cols) = ( n x 2 ) sized array (type `CV_64FC1`) of distortion-free keypoint locations in camera coordinates corresponding to `cv::InputArray p1`
* `std::string method`: Robust method to use: `RANSAC`, `ARRSAC`, or `MLESAC`
* `double threshold`: Threshold in camera coordinates. It can be converted from image coordinates using <img src="https://render.githubusercontent.com/render/math?math=t\!h_{cam} = t\!h_{pix}\frac{4}{\sqrt{2}\left(f_{1,x} %2B f_{1,y} %2B f_{2,x} %2B f_{2,y}\right)}"> with focal lengths *f* of first and second camera in x- and y- direction
* `bool refine`: If true, the found solution is refined using the 8pt algorithm and Pseudo-Huber weights (only for RANSAC and ARRSAC). The used refinement performs additional checks and optimizations. Better performance can be achieved in most cases if the [refinement described below](#interface-pose-linear-refinement) is used instead.

Output variable descriptions:
* `cv::OutputArray E`: Essential matrix in `cv::Mat` format and type `CV_64FC1`
* `cv::OutputArray mask`: Inlier mask of size (rows x cols) = (1 x n) in `cv::Mat` format and type `CV_8UC1`

#### Linear Refinement <a name="interface-pose-linear-refinement"></a>

Robustly estimated essential matrices can be linearly refined using

```
bool poselib::refineEssentialLinear(cv::InputArray p1,
		cv::InputArray p2,
		cv::InputOutputArray E,
		cv::InputOutputArray mask,
		int refineMethod,//a combination of poselib::RefinePostAlg
		size_t & nr_inliers,
		cv::InputOutputArray R = cv::noArray(),
		cv::OutputArray t = cv::noArray(),
		double th = 0.008,
		size_t num_iterative_steps = 4,
		double threshold_multiplier = 2.0,
		double pseudoHuberThreshold_multiplier = 0.1,
		double maxRelativeInlierCntLoss = 0.15);
```
located within [./matchinglib_poselib/source/poselib/include/poselib/pose_linear_refinement.h](./matchinglib_poselib/source/poselib/include/poselib/pose_linear_refinement.h).
It can be included with `#include "poselib/pose_linear_refinement.h"`.

Input variable descriptions:
* `cv::InputArray p1`: (rows x cols) = ( n x 2 ) sized array (type `CV_64FC1`) of distortion-free keypoint locations in camera coordinates corresponding to `cv::InputArray p2`
* `cv::InputArray p2`: (rows x cols) = ( n x 2 ) sized array (type `CV_64FC1`) of distortion-free keypoint locations in camera coordinates corresponding to `cv::InputArray p1`
* `int refineMethod`: Refinement method: A combination of `enum poselib::RefinePostAlg` located in [./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h](./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h).
Refinement algorithms `PR_8PT`, `PR_NISTER`, `PR_STEWENIUS` or `PR_KNEIP` can be combined with one of cost functions `PR_TORR_WEIGHTS`, `PR_PSEUDOHUBER_WEIGHTS`, or `PR_NO_WEIGHTS` (least squares solution) using the `|` (or) operator.
* `double th`:  Threshold in camera coordinates. It can be converted from image coordinates using <img src="https://render.githubusercontent.com/render/math?math=t\!h_{cam} = t\!h_{pix}\frac{4}{\sqrt{2}\left(f_{1,x} %2B f_{1,y} %2B f_{2,x} %2B f_{2,y}\right)}"> with focal lengths *f* of first and second camera in x- and y- direction
* `size_t num_iterative_steps`: Number of refinement steps *s* performed while iteratively changing the threshold to achieve the highest possible inlier count and accuracy
* `double threshold_multiplier`: Threshold multiplier *m* is used to calculate the utilized threshold <img src="https://render.githubusercontent.com/render/math?math=t\!h_{use} = m d - (i %2B 1) t\!h_{step}"> with threshold step size <img src="https://render.githubusercontent.com/render/math?math=t\!h_{step} = \frac{m d - d}{s}">, internal threshold <img src="https://render.githubusercontent.com/render/math?math=d = t\!h_{cam}^{2}">, number of refinement steps *s*, and iteration number *i*. The calculated threshold is used to determine the inliers after the new model was calculated.
* `double pseudoHuberThreshold_multiplier`: Multiplication factor for the threshold `double th` in case the Pseudo-Huber cost function is used as the Pseudo-Huber threshold should typically be smaller than the normal threshold
* `double maxRelativeInlierCntLoss`: Maximum allowed relative inlier loss during iterative refinement

Input/Output variable descriptions:
* `cv::InputOutputArray E`: Input: Essential matrix (`cv::Mat` of type `CV_64FC1`) from robust estimation; Output: Refined Essential matrix
* `cv::InputOutputArray mask`: Inlier mask of size (rows x cols) = (1 x n) in `cv::Mat` format and type `CV_8UC1`
* `cv::InputOutputArray R`: Rotation matrix in case Kneip's Eigen solver is used

Output variable descriptions:
* `cv::OutputArray t`: Translation vector (`cv::Mat` of type `CV_64FC1`) in case Kneip's Eigen solver is used

#### Calculation of R & t

For calculating the pose (i.e. rotation matrix R and translation vector t) and triangulated 3D points use function
```
int poselib::getPoseTriangPts(cv::InputArray E,
					 cv::InputArray p1,
					 cv::InputArray p2,
					 cv::OutputArray R,
					 cv::OutputArray t,
					 cv::OutputArray Q,
					 cv::InputOutputArray mask = cv::noArray(),
					 const double dist = 50.0,
					 bool translatE = false);
```
located within [./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h](./matchinglib_poselib/source/poselib/include/poselib/pose_estim.h).
It can be included with `#include "poselib/pose_estim.h"`.

Input variable descriptions:
* `cv::InputArray E`: Essential matrix (`cv::Mat` of type `CV_64FC1`)
* `cv::InputArray p1`: (rows x cols) = ( n x 2 ) sized array (type `CV_64FC1`) of distortion-free keypoint locations in camera coordinates corresponding to `cv::InputArray p2`
* `cv::InputArray p2`: (rows x cols) = ( n x 2 ) sized array (type `CV_64FC1`) of distortion-free keypoint locations in camera coordinates corresponding to `cv::InputArray p1`
* `double dist`: Threshold on 3D coordinates depth values (z) based on a normalized translation vector (i.e. base length between cameras is 1)
* `bool translatE`: Set to true, if the provided Essential matrix corresponds to a translational essential matrix (R corresponds to identity)

Input/Output variable descriptions:
* `cv::InputOutputArray mask`: Inlier mask of size (rows x cols) = (1 x n) in `cv::Mat` format and type `CV_8UC1`

Output variable descriptions:
* `cv::OutputArray R`: Rotation matrix (`cv::Mat` of type `CV_64FC1`)
* `cv::OutputArray t`: Translation vector (`cv::Mat` of type `CV_64FC1`)
* `cv::OutputArray Q`: Triangulated 3D coordinates